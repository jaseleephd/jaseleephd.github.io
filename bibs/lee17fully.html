<font face="courier",size="4">
    @article{lee17fully,<br>
&emsp;author = {Lee, Jason  and Cho, Kyunghyun  and Hofmann, Thomas },<br>
       title = {Fully Character-Level Neural Machine Translation without<br>
       Explicit Segmentation},<br>
       journal = {Transactions of the Association for Computational<br>
       Linguistics},<br>
       volume = {5},<br>
       year = {2017},<br>
       keywords = {},<br>
       abstract = {Most existing machine translation systems operate at the<br>
       level of words, relying on explicit segmentation to extract tokens. We<br>
       introduce a neural machine translation (NMT) model that maps a source<br>
       character sequence to a target character sequence without any segmentation.<br>
       We employ a character-level convolutional network with max-pooling at the<br>
       encoder to reduce the length of source representation, allowing the model to<br>
       be trained at a speed comparable to subword-level models while capturing<br>
       local regularities. Our character-to-character model outperforms a recently<br>
       proposed baseline with a subword-level encoder on WMT'15 DE-EN and CS-EN,<br>
       and gives comparable performance on FI-EN and RU-EN. We then demonstrate<br>
       that it is possible to share a single character-level encoder across<br>
       multiple languages by training a model on a many-to-one translation task. In<br>
       this multilingual setting, the character-level encoder significantly<br>
       outperforms the subword-level encoder on all the language pairs. We observe<br>
       that on CS-EN, FI-EN and RU-EN, the quality of the multilingual<br>
       character-level translation even surpasses the models specifically trained<br>
       on that language pair alone, both in terms of BLEU score and human<br>
       judgment.},<br>
       issn = {2307-387X},<br>
       url = {https://transacl.org/ojs/index.php/tacl/article/view/1051},<br>
       pages = {365--378}<br>
}<br>
</font>
